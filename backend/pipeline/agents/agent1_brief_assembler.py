"""
Agent 1: Brief Assembler (Blueprint Benny) - LEGENDARY EDITION
==============================================================
Intelligent Semantic Architect that validates logic and strategy,
not just structure.

Features:
- Golden Thread Check: Semantic audit ensuring script addresses pain points
- Chain-of-Thought Confidence: Structured evaluation with actionable critique
- Active Auto-Repair: Fixes issues instead of rejecting
- Parallel Execution: asyncio.gather for minimum latency
- Structured Logging: Full observability with trace_id binding

pip install pydantic anthropic structlog
"""

import asyncio
import hashlib
import json
import re
import uuid
from datetime import datetime
from typing import Optional, Union, Literal, Any
from enum import Enum
from pydantic import BaseModel, Field, field_validator, computed_field, model_validator
from anthropic import AsyncAnthropic
import structlog

# =============================================================================
# STRUCTURED LOGGING SETUP
# =============================================================================

structlog.configure(
    processors=[
        structlog.contextvars.merge_contextvars,
        structlog.processors.add_log_level,
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.JSONRenderer(),
    ],
    wrapper_class=structlog.make_filtering_bound_logger(structlog.logging.INFO),
    context_class=dict,
    logger_factory=structlog.PrintLoggerFactory(),
    cache_logger_on_first_use=True,
)


# =============================================================================
# CONFIGURATION
# =============================================================================

class Settings:
    ANTHROPIC_API_KEY: str = "sk-ant-YOUR-KEY"  # Replace with your key
    MODEL_FAST: str = "claude-3-haiku-20240307"  # <1000 chars, simple tasks
    MODEL_COMPLEX: str = "claude-3-5-sonnet-20241022"  # Complex reasoning
    MAX_COST_PER_BRIEF: float = 0.05  # Cost ceiling
    SEMANTIC_AUDIT_THRESHOLD: float = 0.6  # Min score to pass audit
    CONFIDENCE_THRESHOLD: float = 0.85  # Ready for payment
    AUTO_REPAIR_ENABLED: bool = True

settings = Settings()

DISPOSABLE_DOMAINS = {
    "tempmail.com", "guerrillamail.com", "10minutemail.com",
    "mailinator.com", "yopmail.com", "throwaway.email",
    "fakeinbox.com", "trashmail.com", "temp-mail.org"
}

TIER_PRICES = {"starter": 2500, "professional": 5000, "enterprise": 15000}
VALID_DURATIONS = {30, 60, 120}


# =============================================================================
# ENUMS
# =============================================================================

class CompanySize(str, Enum):
    SOLO = "1"
    SMALL = "2-10"
    MEDIUM = "11-50"
    LARGE = "51-200"
    ENTERPRISE = "201-1000"
    MEGA = "1000+"

class DecisionTimeline(str, Enum):
    IMMEDIATE = "immediate"
    THIS_WEEK = "this_week"
    THIS_MONTH = "this_month"
    THIS_QUARTER = "this_quarter"

class ToneTag(str, Enum):
    PROFESSIONAL = "professional"
    INSPIRING = "inspiring"
    URGENT = "urgent"
    FRIENDLY = "friendly"
    AUTHORITATIVE = "authoritative"

class PaymentTier(str, Enum):
    STARTER = "starter"
    PROFESSIONAL = "professional"
    ENTERPRISE = "enterprise"

class RepairAction(str, Enum):
    TRIMMED = "trimmed"
    SNAPPED = "snapped"
    CLAMPED = "clamped"
    DEFAULTED = "defaulted"
    DEDUPLICATED = "deduplicated"


# =============================================================================
# INPUT SCHEMAS (4 Cards from Website Assistant)
# =============================================================================

class PersonaCard(BaseModel):
    """Card 1: Generated by Profile Patty"""
    persona_name: str = Field(..., min_length=2)
    title: Optional[str] = None
    industry_vertical: str
    company_size: CompanySize
    pain_points: list[str] = Field(..., min_length=1)
    goals: list[str] = Field(..., min_length=1)
    budget_range: str
    decision_timeline: DecisionTimeline

    @field_validator("pain_points", "goals", mode="before")
    @classmethod
    def clean_list(cls, v: list[str]) -> list[str]:
        if isinstance(v, list):
            return [x.strip() for x in v if x and x.strip()]
        return v


class CompetitorAnalysisCard(BaseModel):
    """Card 2: Generated by Battle Card Betty"""
    primary_competitor_name: str
    competitor_weaknesses: list[str] = Field(..., min_length=1)
    our_advantages: list[str] = Field(..., min_length=1)
    positioning_statement: str = Field(..., min_length=10)


class ScriptPreviewCard(BaseModel):
    """Card 3: Generated by Script Sammy"""
    hook_line: str = Field(..., min_length=5)
    problem_statement: str
    solution_presentation: str
    key_benefits: list[str] = Field(..., min_length=1)
    call_to_action: str
    estimated_duration_seconds: int = Field(..., ge=15, le=180)
    tone_tags: list[ToneTag] = Field(default_factory=lambda: [ToneTag.PROFESSIONAL])


class ROICalculatorCard(BaseModel):
    """Card 4: Generated by ROI Randy"""
    investment_amount: int = Field(..., ge=0, le=100000)
    payment_tier: str = Field(..., pattern=r"^(starter|professional|enterprise)$")
    projected_view_count: int = Field(default=0, ge=0)
    projected_lead_count: int = Field(default=0, ge=0)
    projected_revenue: int = Field(default=0, ge=0)
    roi_percentage: float = Field(default=0.0, ge=0)


# =============================================================================
# REPAIR & VALIDATION SCHEMAS
# =============================================================================

class Repair(BaseModel):
    """Single auto-repair action taken"""
    field: str
    original_value: Any
    repaired_value: Any
    action: RepairAction
    reason: str


class ValidationIssue(BaseModel):
    """Validation issue (error or warning)"""
    field: str
    message: str
    severity: Literal["error", "warning"] = "error"
    suggestion: Optional[str] = None


class ValidationResult(BaseModel):
    """Complete validation result with repairs"""
    issues: list[ValidationIssue] = Field(default_factory=list)
    repairs: list[Repair] = Field(default_factory=list)
    repaired_data: dict = Field(default_factory=dict)

    @computed_field
    @property
    def has_errors(self) -> bool:
        return any(i.severity == "error" for i in self.issues)

    @computed_field
    @property
    def error_count(self) -> int:
        return sum(1 for i in self.issues if i.severity == "error")

    @computed_field
    @property
    def warning_count(self) -> int:
        return sum(1 for i in self.issues if i.severity == "warning")


# =============================================================================
# SEMANTIC AUDIT SCHEMAS
# =============================================================================

class GoldenThreadResult(BaseModel):
    """Result of semantic audit: Does script address pain points?"""
    matches: bool
    alignment_score: float = Field(..., ge=0.0, le=1.0)
    addressed_pain_points: list[str] = Field(default_factory=list)
    missed_pain_points: list[str] = Field(default_factory=list)
    reasoning: str
    recommendations: list[str] = Field(default_factory=list)


class ConfidenceEvaluation(BaseModel):
    """Chain-of-Thought confidence evaluation"""
    clarity: float = Field(..., ge=0.0, le=1.0, description="Message clarity score")
    alignment: float = Field(..., ge=0.0, le=1.0, description="Persona-script alignment")
    viability: float = Field(..., ge=0.0, le=1.0, description="Commercial viability")
    hook_strength: float = Field(..., ge=0.0, le=1.0, description="Hook effectiveness")
    cta_strength: float = Field(..., ge=0.0, le=1.0, description="CTA effectiveness")
    final_score: float = Field(..., ge=0.0, le=1.0, description="Weighted final score")
    critique: str = Field(..., description="Actionable improvement suggestions")
    strengths: list[str] = Field(default_factory=list)
    weaknesses: list[str] = Field(default_factory=list)


# =============================================================================
# OUTPUT SCHEMAS
# =============================================================================

class BriefValidationError(BaseModel):
    """Returned when brief fails validation"""
    session_id: str
    issues: list[ValidationIssue]
    repairs_attempted: list[Repair] = Field(default_factory=list)
    confidence_score: float
    semantic_audit: Optional[GoldenThreadResult] = None
    suggestions: list[str] = Field(default_factory=list)
    timestamp: datetime = Field(default_factory=datetime.utcnow)

    @computed_field
    @property
    def is_recoverable(self) -> bool:
        error_count = sum(1 for i in self.issues if i.severity == "error")
        return error_count <= 2


class CreativeBrief(BaseModel):
    """Complete brief ready for payment - includes critique for improvement"""
    brief_id: str
    session_id: str
    business_name: str
    contact_email: str
    industry: str
    company_size: str
    duration_seconds: int
    tone_tags: list[str]
    key_message: str
    call_to_action: str
    target_audience: str
    unique_selling_proposition: str
    pain_points: list[str]
    payment_tier: PaymentTier
    quoted_price: int

    # Quality metrics (NEW)
    confidence_score: float
    confidence_evaluation: ConfidenceEvaluation
    semantic_audit: GoldenThreadResult
    repairs_applied: list[Repair] = Field(default_factory=list)

    created_at: datetime = Field(default_factory=datetime.utcnow)

    @computed_field
    @property
    def is_ready_for_payment(self) -> bool:
        return (
            self.confidence_score >= settings.CONFIDENCE_THRESHOLD
            and self.semantic_audit.matches
        )

    @computed_field
    @property
    def quality_grade(self) -> str:
        """A-F grade based on confidence"""
        if self.confidence_score >= 0.95:
            return "A+"
        elif self.confidence_score >= 0.90:
            return "A"
        elif self.confidence_score >= 0.85:
            return "B+"
        elif self.confidence_score >= 0.80:
            return "B"
        elif self.confidence_score >= 0.70:
            return "C"
        elif self.confidence_score >= 0.60:
            return "D"
        return "F"

    @staticmethod
    def generate_brief_id(session_id: str) -> str:
        h = hashlib.sha256(f"brief:{session_id}".encode()).hexdigest()[:8].upper()
        return f"BRF-{h}"


# =============================================================================
# BRIEF ASSEMBLER AGENT - LEGENDARY EDITION
# =============================================================================

class BriefAssembler:
    """
    Legendary Brief Assembler: Intelligent Semantic Architect
    
    Features:
    - Golden Thread: Semantic audit ensuring script addresses pain points
    - Chain-of-Thought: Structured confidence with actionable critique
    - Auto-Repair: Fixes minor issues instead of rejecting
    - Parallel Execution: Concurrent LLM calls for speed
    - Structured Logging: Full observability
    
    Example:
        assembler = BriefAssembler("sk-ant-xxx")
        result = await assembler.process_cards(
            persona_card={...}, competitor_card={...},
            script_card={...}, roi_card={...},
            session_id="sess-001", user_email="x@y.com", business_name="Acme"
        )
        
        if isinstance(result, CreativeBrief):
            print(f"Quality: {result.quality_grade}")
            print(f"Critique: {result.confidence_evaluation.critique}")
    """

    def __init__(self, api_key: str = None):
        self.anthropic = AsyncAnthropic(api_key=api_key or settings.ANTHROPIC_API_KEY)
        self._base_logger = structlog.get_logger()

    def _get_logger(self, session_id: str, trace_id: str = None):
        """Get logger bound with session context"""
        return self._base_logger.bind(
            session_id=session_id,
            trace_id=trace_id or str(uuid.uuid4()),
            agent="brief_assembler",
            version="2.0-legendary"
        )

    # =========================================================================
    # MAIN ENTRY POINT
    # =========================================================================

    async def process_cards(
        self,
        persona_card: dict,
        competitor_card: dict,
        script_card: dict,
        roi_card: dict,
        session_id: str,
        user_email: str,
        business_name: str,
        trace_id: str = None,
    ) -> Union[CreativeBrief, BriefValidationError]:
        """
        Process 4 cards with parallel semantic analysis.
        Returns validated CreativeBrief or detailed error.
        """
        trace_id = trace_id or str(uuid.uuid4())
        log = self._get_logger(session_id, trace_id)

        log.info("processing_started", business_name=business_name)
        start_time = datetime.utcnow()

        try:
            # Step 1: Parse and auto-repair cards
            log.info("parsing_cards")
            repaired_cards = self._parse_and_repair_cards(
                persona_card, competitor_card, script_card, roi_card,
                user_email, business_name, log
            )

            persona = repaired_cards["persona"]
            competitor = repaired_cards["competitor"]
            script = repaired_cards["script"]
            roi = repaired_cards["roi"]
            validation_result = repaired_cards["validation"]

            # Log repairs
            if validation_result.repairs:
                log.info("auto_repairs_applied",
                         repair_count=len(validation_result.repairs),
                         repairs=[r.model_dump() for r in validation_result.repairs])

            # Step 2: Check for blocking errors
            if validation_result.has_errors:
                log.warning("validation_failed",
                           error_count=validation_result.error_count,
                           errors=[i.model_dump() for i in validation_result.issues if i.severity == "error"])

                return BriefValidationError(
                    session_id=session_id,
                    issues=validation_result.issues,
                    repairs_attempted=validation_result.repairs,
                    confidence_score=self._calc_base_confidence(validation_result),
                    suggestions=[i.suggestion for i in validation_result.issues if i.suggestion],
                )

            # Step 3: PARALLEL EXECUTION - Semantic audit + Confidence scoring
            log.info("parallel_analysis_started")

            semantic_task = self._semantic_audit(persona, script, log)
            confidence_task = self._llm_confidence_cot(persona, competitor, script, roi, log)

            semantic_result, confidence_result = await asyncio.gather(
                semantic_task, confidence_task, return_exceptions=True
            )

            # Handle exceptions
            if isinstance(semantic_result, Exception):
                log.error("semantic_audit_failed", error=str(semantic_result))
                semantic_result = self._default_semantic_result()

            if isinstance(confidence_result, Exception):
                log.error("confidence_scoring_failed", error=str(confidence_result))
                confidence_result = self._default_confidence_result()

            log.info("parallel_analysis_complete",
                     semantic_matches=semantic_result.matches,
                     semantic_score=semantic_result.alignment_score,
                     confidence_score=confidence_result.final_score)

            # Step 4: Apply semantic penalty if golden thread broken
            final_confidence = confidence_result.final_score
            if not semantic_result.matches:
                penalty = 0.15  # 15% penalty for broken golden thread
                final_confidence = max(0.0, final_confidence - penalty)
                log.warning("golden_thread_broken",
                           original_score=confidence_result.final_score,
                           penalty=penalty,
                           final_score=final_confidence)

            # Step 5: Assemble final brief
            brief = self._assemble_brief(
                persona=persona,
                competitor=competitor,
                script=script,
                roi=roi,
                session_id=session_id,
                user_email=repaired_cards["email"],
                business_name=repaired_cards["business_name"],
                confidence=final_confidence,
                confidence_eval=confidence_result,
                semantic_audit=semantic_result,
                repairs=validation_result.repairs,
            )

            elapsed = (datetime.utcnow() - start_time).total_seconds()
            log.info("processing_complete",
                     brief_id=brief.brief_id,
                     quality_grade=brief.quality_grade,
                     ready_for_payment=brief.is_ready_for_payment,
                     elapsed_seconds=elapsed)

            return brief

        except Exception as e:
            log.exception("processing_error", error=str(e))
            raise

    # =========================================================================
    # STEP 1: PARSING & AUTO-REPAIR
    # =========================================================================

    def _parse_and_repair_cards(
        self,
        persona_card: dict,
        competitor_card: dict,
        script_card: dict,
        roi_card: dict,
        user_email: str,
        business_name: str,
        log: Any,
    ) -> dict:
        """Parse cards with auto-repair for minor issues."""
        validation = ValidationResult()
        repairs = []

        # --- Email Auto-Repair ---
        original_email = user_email
        repaired_email = user_email.strip().lower() if user_email else ""

        if original_email != repaired_email and repaired_email:
            repairs.append(Repair(
                field="user_email",
                original_value=original_email,
                repaired_value=repaired_email,
                action=RepairAction.TRIMMED,
                reason="Trimmed whitespace and lowercased email"
            ))

        if not repaired_email or "@" not in repaired_email:
            validation.issues.append(ValidationIssue(
                field="user_email",
                message="Valid email address is required",
                severity="error",
                suggestion="Provide a valid email like user@company.com"
            ))
        elif repaired_email.split("@")[1] in DISPOSABLE_DOMAINS:
            validation.issues.append(ValidationIssue(
                field="user_email",
                message="Disposable email domains are not allowed",
                severity="error",
                suggestion="Use a business or personal email domain"
            ))

        # --- Business Name Auto-Repair ---
        original_biz = business_name
        repaired_biz = business_name.strip() if business_name else ""

        if original_biz != repaired_biz and repaired_biz:
            repairs.append(Repair(
                field="business_name",
                original_value=original_biz,
                repaired_value=repaired_biz,
                action=RepairAction.TRIMMED,
                reason="Trimmed whitespace from business name"
            ))

        if not repaired_biz or len(repaired_biz) < 2:
            validation.issues.append(ValidationIssue(
                field="business_name",
                message="Business name is required (min 2 characters)",
                severity="error",
                suggestion="Enter your company or brand name"
            ))

        # --- Parse PersonaCard ---
        try:
            persona = PersonaCard.model_validate(persona_card)
            if len(persona.pain_points) < 2:
                validation.issues.append(ValidationIssue(
                    field="pain_points",
                    message="At least 2 pain points recommended for effective messaging",
                    severity="warning",
                    suggestion="Add more specific customer pain points"
                ))
        except Exception as e:
            validation.issues.append(ValidationIssue(
                field="persona_card",
                message=f"Invalid persona card: {str(e)}",
                severity="error"
            ))
            persona = None

        # --- Parse CompetitorCard ---
        try:
            competitor = CompetitorAnalysisCard.model_validate(competitor_card)
            if len(competitor.our_advantages) < 2:
                validation.issues.append(ValidationIssue(
                    field="our_advantages",
                    message="At least 2 competitive advantages recommended",
                    severity="warning",
                    suggestion="What makes you better than competitors?"
                ))
        except Exception as e:
            validation.issues.append(ValidationIssue(
                field="competitor_card",
                message=f"Invalid competitor card: {str(e)}",
                severity="error"
            ))
            competitor = None

        # --- Parse ScriptCard with Duration Auto-Repair ---
        try:
            script_data = script_card.copy()

            # Auto-snap duration to nearest valid tier
            original_duration = script_data.get("estimated_duration_seconds", 60)
            if original_duration not in VALID_DURATIONS:
                snapped_duration = min(VALID_DURATIONS, key=lambda x: abs(x - original_duration))
                script_data["estimated_duration_seconds"] = snapped_duration
                repairs.append(Repair(
                    field="estimated_duration_seconds",
                    original_value=original_duration,
                    repaired_value=snapped_duration,
                    action=RepairAction.SNAPPED,
                    reason=f"Snapped {original_duration}s to nearest valid duration {snapped_duration}s"
                ))

            script = ScriptPreviewCard.model_validate(script_data)

            if len(script.hook_line) < 15:
                validation.issues.append(ValidationIssue(
                    field="hook_line",
                    message="Hook line is quite short",
                    severity="warning",
                    suggestion="A compelling hook should be 15-50 characters"
                ))

        except Exception as e:
            validation.issues.append(ValidationIssue(
                field="script_card",
                message=f"Invalid script card: {str(e)}",
                severity="error"
            ))
            script = None

        # --- Parse ROICard with Clamping Auto-Repair ---
        try:
            roi_data = roi_card.copy()

            # Clamp negative values to 0
            for field in ["projected_view_count", "projected_lead_count", "projected_revenue"]:
                val = roi_data.get(field, 0)
                if val < 0:
                    roi_data[field] = 0
                    repairs.append(Repair(
                        field=field,
                        original_value=val,
                        repaired_value=0,
                        action=RepairAction.CLAMPED,
                        reason=f"Clamped negative {field} to 0"
                    ))

            # Validate price matches tier
            tier = roi_data.get("payment_tier", "starter")
            expected_price = TIER_PRICES.get(tier, 2500)
            actual_price = roi_data.get("investment_amount", 0)

            if actual_price != expected_price:
                roi_data["investment_amount"] = expected_price
                repairs.append(Repair(
                    field="investment_amount",
                    original_value=actual_price,
                    repaired_value=expected_price,
                    action=RepairAction.SNAPPED,
                    reason=f"Corrected price to ${expected_price} for {tier} tier"
                ))

            roi = ROICalculatorCard.model_validate(roi_data)

        except Exception as e:
            validation.issues.append(ValidationIssue(
                field="roi_card",
                message=f"Invalid ROI card: {str(e)}",
                severity="error"
            ))
            roi = None

        validation.repairs = repairs

        return {
            "persona": persona,
            "competitor": competitor,
            "script": script,
            "roi": roi,
            "email": repaired_email,
            "business_name": repaired_biz,
            "validation": validation,
        }

    def _calc_base_confidence(self, validation: ValidationResult) -> float:
        """Calculate base confidence from validation result."""
        base = 1.0
        base -= validation.error_count * 0.25
        base -= validation.warning_count * 0.05
        return max(0.0, min(1.0, base))

    # =========================================================================
    # STEP 2: SEMANTIC AUDIT (Golden Thread)
    # =========================================================================

    async def _semantic_audit(
        self,
        persona: PersonaCard,
        script: ScriptPreviewCard,
        log: Any,
    ) -> GoldenThreadResult:
        """
        Check if script addresses persona's pain points (Golden Thread).
        Returns detailed alignment analysis.
        """
        log.info("semantic_audit_started")

        prompt = f"""Analyze if this commercial video script addresses the target persona's pain points.

PERSONA PAIN POINTS:
{json.dumps(persona.pain_points, indent=2)}

PERSONA GOALS:
{json.dumps(persona.goals, indent=2)}

SCRIPT CONTENT:
- Hook: {script.hook_line}
- Problem Statement: {script.problem_statement}
- Solution: {script.solution_presentation}
- Benefits: {json.dumps(script.key_benefits)}
- CTA: {script.call_to_action}

TASK: Evaluate alignment between persona needs and script messaging.

Return ONLY valid JSON (no markdown, no explanation):
{{
    "matches": true/false (does script meaningfully address at least 50% of pain points?),
    "alignment_score": 0.0-1.0 (overall alignment quality),
    "addressed_pain_points": ["pain point 1 that IS addressed", ...],
    "missed_pain_points": ["pain point that is NOT addressed", ...],
    "reasoning": "2-3 sentence explanation of the alignment analysis",
    "recommendations": ["specific actionable improvement 1", "improvement 2"]
}}"""

        try:
            response = await self.anthropic.messages.create(
                model=settings.MODEL_FAST,  # Haiku for speed
                max_tokens=500,
                messages=[{"role": "user", "content": prompt}]
            )

            text = response.content[0].text.strip()
            # Clean potential markdown
            if text.startswith("```"):
                text = re.sub(r"```json?\s*", "", text)
                text = re.sub(r"```\s*$", "", text)

            data = json.loads(text)
            result = GoldenThreadResult(**data)

            log.info("semantic_audit_complete",
                     matches=result.matches,
                     alignment_score=result.alignment_score,
                     addressed=len(result.addressed_pain_points),
                     missed=len(result.missed_pain_points))

            return result

        except Exception as e:
            log.warning("semantic_audit_fallback", error=str(e))
            return self._default_semantic_result()

    def _default_semantic_result(self) -> GoldenThreadResult:
        """Fallback semantic result when LLM fails."""
        return GoldenThreadResult(
            matches=True,  # Assume pass on failure
            alignment_score=0.75,
            addressed_pain_points=[],
            missed_pain_points=[],
            reasoning="Semantic audit could not be completed. Manual review recommended.",
            recommendations=["Review script alignment with persona pain points manually."]
        )

    # =========================================================================
    # STEP 3: CHAIN-OF-THOUGHT CONFIDENCE SCORING
    # =========================================================================

    async def _llm_confidence_cot(
        self,
        persona: PersonaCard,
        competitor: CompetitorAnalysisCard,
        script: ScriptPreviewCard,
        roi: ROICalculatorCard,
        log: Any,
    ) -> ConfidenceEvaluation:
        """
        Chain-of-Thought confidence evaluation with structured critique.
        """
        log.info("confidence_evaluation_started")

        brief_summary = f"""
PERSONA: {persona.persona_name} - {persona.title or 'Decision Maker'}
INDUSTRY: {persona.industry_vertical} | SIZE: {persona.company_size.value}
PAIN POINTS: {', '.join(persona.pain_points[:3])}

COMPETITIVE POSITIONING: {competitor.positioning_statement}
OUR ADVANTAGES: {', '.join(competitor.our_advantages[:3])}

SCRIPT:
- Hook: "{script.hook_line}"
- Problem: "{script.problem_statement[:100]}..."
- Solution: "{script.solution_presentation[:100]}..."
- Benefits: {', '.join(script.key_benefits[:3])}
- CTA: "{script.call_to_action}"
- Duration: {script.estimated_duration_seconds}s
- Tone: {', '.join(t.value for t in script.tone_tags)}

INVESTMENT: ${roi.investment_amount} ({roi.payment_tier})
PROJECTED ROI: {roi.roi_percentage}%
"""

        prompt = f"""You are a senior creative director evaluating a commercial video brief for production readiness.

{brief_summary}

Evaluate this brief on 5 dimensions (0.0 to 1.0 each):
1. CLARITY: Is the message clear and focused?
2. ALIGNMENT: Does the script match the persona's needs?
3. VIABILITY: Is this commercially viable at this price point?
4. HOOK_STRENGTH: Will the hook capture attention in 3 seconds?
5. CTA_STRENGTH: Is the call-to-action compelling and clear?

Then provide:
- FINAL_SCORE: Weighted average (clarity 0.2, alignment 0.3, viability 0.2, hook 0.15, cta 0.15)
- CRITIQUE: 2-3 sentences of specific, actionable feedback
- STRENGTHS: Top 2-3 things done well
- WEAKNESSES: Top 2-3 things to improve

Return ONLY valid JSON (no markdown):
{{
    "clarity": 0.0-1.0,
    "alignment": 0.0-1.0,
    "viability": 0.0-1.0,
    "hook_strength": 0.0-1.0,
    "cta_strength": 0.0-1.0,
    "final_score": 0.0-1.0,
    "critique": "Specific actionable feedback...",
    "strengths": ["strength 1", "strength 2"],
    "weaknesses": ["weakness 1", "weakness 2"]
}}"""

        try:
            # Use Sonnet for complex reasoning
            response = await self.anthropic.messages.create(
                model=settings.MODEL_COMPLEX,
                max_tokens=600,
                messages=[{"role": "user", "content": prompt}]
            )

            text = response.content[0].text.strip()
            if text.startswith("```"):
                text = re.sub(r"```json?\s*", "", text)
                text = re.sub(r"```\s*$", "", text)

            data = json.loads(text)
            result = ConfidenceEvaluation(**data)

            log.info("confidence_evaluation_complete",
                     clarity=result.clarity,
                     alignment=result.alignment,
                     viability=result.viability,
                     hook_strength=result.hook_strength,
                     cta_strength=result.cta_strength,
                     final_score=result.final_score)

            return result

        except Exception as e:
            log.warning("confidence_evaluation_fallback", error=str(e))
            return self._default_confidence_result()

    def _default_confidence_result(self) -> ConfidenceEvaluation:
        """Fallback confidence when LLM fails."""
        return ConfidenceEvaluation(
            clarity=0.75,
            alignment=0.75,
            viability=0.75,
            hook_strength=0.70,
            cta_strength=0.70,
            final_score=0.73,
            critique="Automated evaluation could not be completed. Manual review recommended.",
            strengths=["Brief structure is complete"],
            weaknesses=["Unable to evaluate content quality automatically"]
        )

    # =========================================================================
    # STEP 4: ASSEMBLY
    # =========================================================================

    def _assemble_brief(
        self,
        persona: PersonaCard,
        competitor: CompetitorAnalysisCard,
        script: ScriptPreviewCard,
        roi: ROICalculatorCard,
        session_id: str,
        user_email: str,
        business_name: str,
        confidence: float,
        confidence_eval: ConfidenceEvaluation,
        semantic_audit: GoldenThreadResult,
        repairs: list[Repair],
    ) -> CreativeBrief:
        """Assemble final brief with all quality metrics."""

        tier_map = {
            "starter": PaymentTier.STARTER,
            "professional": PaymentTier.PROFESSIONAL,
            "enterprise": PaymentTier.ENTERPRISE
        }

        key_message = f"{script.solution_presentation} {' '.join(script.key_benefits[:2])}"
        target_audience = f"{persona.title or 'Leaders'} in {persona.industry_vertical} ({persona.company_size.value} employees)"
        usp = f"{competitor.positioning_statement}. Advantages: {', '.join(competitor.our_advantages[:2])}"

        return CreativeBrief(
            brief_id=CreativeBrief.generate_brief_id(session_id),
            session_id=session_id,
            business_name=business_name,
            contact_email=user_email,
            industry=persona.industry_vertical,
            company_size=persona.company_size.value,
            duration_seconds=script.estimated_duration_seconds,
            tone_tags=[t.value for t in script.tone_tags],
            key_message=key_message[:500],
            call_to_action=script.call_to_action,
            target_audience=target_audience[:300],
            unique_selling_proposition=usp[:500],
            pain_points=persona.pain_points[:5],
            payment_tier=tier_map.get(roi.payment_tier, PaymentTier.STARTER),
            quoted_price=roi.investment_amount,
            confidence_score=confidence,
            confidence_evaluation=confidence_eval,
            semantic_audit=semantic_audit,
            repairs_applied=repairs,
        )


# =============================================================================
# EXAMPLE USAGE
# =============================================================================

async def main():
    """Demo the legendary Brief Assembler."""

    assembler = BriefAssembler()

    # Test case with intentional issues (to demo auto-repair)
    result = await assembler.process_cards(
        persona_card={
            "persona_name": "Tech-Savvy Tim",
            "title": "VP of Engineering",
            "industry_vertical": "SaaS",
            "company_size": "11-50",
            "pain_points": ["High CAC", "Low conversion rates", "Manual processes"],
            "goals": ["Reduce CAC by 30%", "Automate lead gen"],
            "budget_range": "$5K-$15K",
            "decision_timeline": "this_month",
        },
        competitor_card={
            "primary_competitor_name": "SlowCorp",
            "competitor_weaknesses": ["Slow support", "Expensive"],
            "our_advantages": ["24/7 AI support", "50% cheaper", "Self-service"],
            "positioning_statement": "The smarter, faster, more affordable alternative",
        },
        script_card={
            "hook_line": "Stop losing customers to slow support",
            "problem_statement": "Every minute of wait time costs you customers and revenue",
            "solution_presentation": "Our AI responds instantly, 24/7, at a fraction of the cost",
            "key_benefits": ["Instant response", "50% cost reduction", "Happier customers"],
            "call_to_action": "Start your free trial today",
            "estimated_duration_seconds": 45,  # Will be snapped to 60
            "tone_tags": ["professional", "urgent"],
        },
        roi_card={
            "investment_amount": 4500,  # Will be snapped to 5000
            "payment_tier": "professional",
            "projected_view_count": -100,  # Will be clamped to 0
            "projected_lead_count": 500,
            "projected_revenue": 100000,
            "roi_percentage": 1900,
        },
        session_id="demo-session-001",
        user_email="  Tim@Example.COM  ",  # Will be trimmed/lowercased
        business_name="TechCorp Inc",
    )

    print("\n" + "=" * 60)
    print("LEGENDARY BRIEF ASSEMBLER - RESULTS")
    print("=" * 60)

    if isinstance(result, BriefValidationError):
        print(f"\n‚ùå VALIDATION FAILED")
        print(f"   Errors: {len([i for i in result.issues if i.severity == 'error'])}")
        print(f"   Warnings: {len([i for i in result.issues if i.severity == 'warning'])}")
        for issue in result.issues:
            print(f"   [{issue.severity.upper()}] {issue.field}: {issue.message}")
    else:
        print(f"\n‚úÖ BRIEF CREATED: {result.brief_id}")
        print(f"   Quality Grade: {result.quality_grade}")
        print(f"   Confidence: {result.confidence_score:.2%}")
        print(f"   Ready for Payment: {result.is_ready_for_payment}")

        print(f"\nüìä CONFIDENCE BREAKDOWN:")
        eval = result.confidence_evaluation
        print(f"   Clarity:      {eval.clarity:.2f}")
        print(f"   Alignment:    {eval.alignment:.2f}")
        print(f"   Viability:    {eval.viability:.2f}")
        print(f"   Hook:         {eval.hook_strength:.2f}")
        print(f"   CTA:          {eval.cta_strength:.2f}")

        print(f"\nüí° CRITIQUE:")
        print(f"   {eval.critique}")

        print(f"\nüéØ GOLDEN THREAD AUDIT:")
        audit = result.semantic_audit
        print(f"   Matches: {audit.matches}")
        print(f"   Alignment Score: {audit.alignment_score:.2%}")
        if audit.addressed_pain_points:
            print(f"   ‚úì Addressed: {', '.join(audit.addressed_pain_points[:2])}")
        if audit.missed_pain_points:
            print(f"   ‚úó Missed: {', '.join(audit.missed_pain_points[:2])}")

        if result.repairs_applied:
            print(f"\nüîß AUTO-REPAIRS APPLIED ({len(result.repairs_applied)}):")
            for repair in result.repairs_applied:
                print(f"   {repair.field}: {repair.original_value} ‚Üí {repair.repaired_value}")
                print(f"      ({repair.reason})")

        if eval.strengths:
            print(f"\n‚ú® STRENGTHS:")
            for s in eval.strengths[:3]:
                print(f"   ‚Ä¢ {s}")

        if eval.weaknesses:
            print(f"\n‚ö†Ô∏è  IMPROVEMENTS:")
            for w in eval.weaknesses[:3]:
                print(f"   ‚Ä¢ {w}")


if __name__ == "__main__":
    asyncio.run(main())
